{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "\n",
    "# Scraping BBC\n",
    "def get_article_urls(section_url, base_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    try:\n",
    "        response = requests.get(section_url, headers = headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        links = []\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if href.startswith('/') and not href.endswith('live'):\n",
    "                full_url = f'{base_url}{href}'\n",
    "                links.append(full_url)\n",
    "            elif href.startswith(base_url):\n",
    "                links.append(href)\n",
    "\n",
    "        return list(set(links))\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URLs from {section_url}: {e}\")\n",
    "        return []\n",
    "    \n",
    "def scrape_article(url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title = soup.title.string if soup.title else ''\n",
    "        paragraphs = soup.find_all('p')\n",
    "        content = ' '.join([paragraph.get_text() for paragraph in paragraphs])\n",
    "\n",
    "        return {'url': url, 'title': title, 'content': content}\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping article {url}: {e}\")\n",
    "        return {'url': url, 'title': '', 'content': ''}\n",
    "\n",
    "# List of BBC sections and their subsections\n",
    "bbc_sections = [\n",
    "    'https://www.bbc.com/news',\n",
    "    'https://www.bbc.com/news/topics/c2vdnvdg6xxt', \n",
    "    'https://www.bbc.com/news/war-in-ukraine', \n",
    "    'https://www.bbc.com/news/topics/crggn4j2lm0t', \n",
    "    'https://www.bbc.com/news/us-canada', \n",
    "    'https://www.bbc.com/news/uk', \n",
    "    'https://www.bbc.com/news/world/africa', \n",
    "    'https://www.bbc.com/news/world/asia', \n",
    "    'https://www.bbc.com/news/world/australia', \n",
    "    'https://www.bbc.com/news/world/europe', \n",
    "    'https://www.bbc.com/news/world/latin_america', \n",
    "    'https://www.bbc.com/news/world/middle_east', \n",
    "    'https://www.bbc.com/news/in_pictures', \n",
    "    'https://www.bbc.com/news/reality_check',\n",
    "    'https://www.bbc.com/sport',\n",
    "    'https://www.bbc.com/business', \n",
    "    'https://www.bbc.com/innovation', \n",
    "    'https://www.bbc.com/culture', \n",
    "    'https://www.bbc.com/travel', \n",
    "    'https://www.bbc.com/future-planet'\n",
    "]\n",
    "\n",
    "def fetch_urls(sections, base_url):\n",
    "    all_urls = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(get_article_urls, section, base_url): section for section in sections}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            section = future_to_url[future]\n",
    "            try:\n",
    "                urls = future.result()\n",
    "                all_urls.extend(urls)\n",
    "            except Exception as e:\n",
    "                print(f\"Error getting URLs from {section}: {e}\")\n",
    "    return list(set(all_urls))\n",
    "\n",
    "def fetch_articles(urls):\n",
    "    articles = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(scrape_article, url): url for url in urls}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                article_data = future.result()\n",
    "                articles.append(article_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error scraping article {url}: {e}\")\n",
    "    return articles\n",
    "\n",
    "# Fetch all URLs for BBC``\n",
    "bbc_article_urls = fetch_urls(bbc_sections, 'https://www.bbc.com')\n",
    "\n",
    "# Scrape articles concurrently\n",
    "articles = fetch_articles(bbc_article_urls)\n",
    "\n",
    "# Convert the list of dictionaries to a pandas DataFrame\n",
    "bbc_df = pd.DataFrame(articles)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "bbc_df.to_csv('bbc_articles.csv', index = False)\n",
    "\n",
    "# Print a sample of the scraped articles\n",
    "print(bbc_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
